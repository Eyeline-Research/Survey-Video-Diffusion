# Survey of Video Diffusion Models: Foundations, Implementations, and Applications

<p align="center">
  <div style="text-align:center; font-size: 18px;">
      <p>
      <a href="mailto:yimu.wang@uwaterloo.ca">Yimu Wang<sup>1,*</sup></a>, 
      <a href="mailto:x827liu@uwaterloo.ca" >Xuye Liu<sup>1,*</sup>,</a>
      <a href="mailto:w3pang@uwaterloo.ca" >Wei Pang<sup>1,*</sup>,</a>
      <a href="mailto:li.ma@scanlinevfx.com" >Li Ma<sup>2,*</sup>,</a>
      <a href="mailto:shuai@cs.duke.edu" >Shuai Yuan<sup>3,*</sup>,</a>
      <a href="mailto:debevec@netflix.com" >Paul Debevec<sup>2,*</sup>,</a>
      <a href="mailto:ning.yu@scanlinevfx.com" >Ning Yu<sup>2,†</sup></a>
      </p>
  </div>
</p>

<p align="center">
  <sup>1</sup>University of Waterloo, <sup>2</sup>Netflix Eyeline Studios, <sup>3</sup>Duke University, <sup>†</sup>Contributed Equally<br> <sup>†</sup>Corresponding Author
</p>
- [News] <span style="color:red;"> **Our survey is on Arxiv now.**</span>


## Table of Contents 

- [Survey of Video Diffusion Models: Foundations, Implementations, and Applications](#survey-of-video-diffusion-models-foundations-implementations-and-applications)
  - [Contact](#contact)
  - [Table of Contents](#table-of-contents)
      - [Table samples](#table-samples)
- [Foundation](#foundation)
  - [Video generative paradigm](#video-generative-paradigm)
    - [GAN video models](#gan-video-models)
    - [Auto-regressive video models](#auto-regressive-video-models)
    - [Video diffusion models](#video-diffusion-models)
    - [Auto-regressive video diffusion models](#auto-regressive-video-diffusion-models)
  - [Learning foundation](#learning-foundation)
    - [Denoising diffusion probabilistic models (DDPM)](#denoising-diffusion-probabilistic-models-ddpm)
    - [Denoising diffusion implicit models (DDIM)](#denoising-diffusion-implicit-models-ddim)
    - [Elucidated diffusion models (EDM)](#elucidated-diffusion-models-edm)
    - [Flow matching and rectified flow](#flow-matching-and-rectified-flow)
    - [Learning from feedback and reward models](#learning-from-feedback-and-reward-models)
    - [One-shot and few-shot learning](#one-shot-and-few-shot-learning)
    - [Training-free methods](#training-free-methods)
    - [Token learning](#token-learning)
  - [Guidance](#guidance)
    - [Classifier guidance](#classifier-guidance)
    - [Classifier-free guidance](#classifier-free-guidance)
  - [Diffusion model frameworks](#diffusion-model-frameworks)
    - [Pixel diffusion and latent diffusion](#pixel-diffusion-and-latent-diffusion)
    - [Optical-flow-based diffusion models](#optical-flow-based-diffusion-models)
    - [Noise scheduling](#noise-scheduling)
    - [Agent-based diffusion models](#agent-based-diffusion-models)
  - [Architecture](#architecture)
    - [UNet](#unet)
    - [Diffusion transformers](#diffusion-transformers)
    - [VAE for latent space compression](#vae-for-latent-space-compression)
    - [Text encoder](#text-encoder)
- [Implementation](#implementation)
  - [Datasets](#datasets)
  - [Training engineering](#training-engineering)
  - [Evaluation metrics and benchmarking findings](#evaluation-metrics-and-benchmarking-findings)
  - [Industry solutions](#industry-solutions)
- [Applications](#applications)
  - [Conditions](#conditions)
    - [Image condition](#image-condition)
    - [Spatial condition](#spatial-condition)
    - [Camera parameter condition](#camera-parameter-condition)
    - [Audio condition](#audio-condition)
    - [High-level video condition](#high-level-video-condition)
    - [Other conditions](#other-conditions)
  - [Enhancement](#enhancement)
    - [Video denoising and deblurring](#video-denoising-and-deblurring)
    - [Video inpainting](#video-inpainting)
    - [Video interpolation and extrapolation/prediction](#video-interpolation-and-extrapolationprediction)
    - [Video super-resolution](#video-super-resolution)
    - [Combining multiple video enhancement tasks](#combining-multiple-video-enhancement-tasks)
  - [Personalization](#personalization)
  - [Consistency](#consistency)
  - [Long video](#long-video)
  - [3D-aware video diffusion](#3d-aware-video-diffusion)
    - [Training on 3D dataset](#training-on-3d-dataset)
    - [Architecture for 3D diffusion models](#architecture-for-3d-diffusion-models)
    - [Camera conditioning](#camera-conditioning)
    - [Inference-time tricks](#inference-time-tricks)
- [Benefits to other domains](#benefits-to-other-domains)
  - [Video representation learning](#video-representation-learning)
  - [Video retrieval](#video-retrieval)
  - [Video QA and captioning](#video-qa-and-captioning)
  - [3D and 4D generation](#3d-and-4d-generation)
    - [Video diffusion for 3D generation](#video-diffusion-for-3d-generation)
    - [Video diffusion for 4D generation](#video-diffusion-for-4d-generation)

#### Table samples
| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [Identity-Preserving Text-to-Video Generation by Frequency Decomposition](https://arxiv.org/abs/2411.17440) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.17440) |[![Star](https://img.shields.io/github/stars/PKU-YuanGroup/ConsisID.svg?style=social&label=Star)](https://github.com/PKU-YuanGroup/ConsisID)|[![Website](https://img.shields.io/badge/Website-9cf)](https://pku-yuangroup.github.io/ConsisID/) | Nov., 2024
| []() | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]() | [![Star](https://img.shields.io/github/stars/{RepoOwnerName}/{RepoName}.svg?style=social&label=Star)]()|[![Website](https://img.shields.io/badge/Website-9cf)]() | ICLR 2024 |

Remember to update the repo name and owner name in the above table.


# Foundation

## Video generative paradigm

### GAN video models

### Auto-regressive video models

### Video diffusion models

### Auto-regressive video diffusion models

## Learning foundation

### Denoising diffusion probabilistic models (DDPM)

### Denoising diffusion implicit models (DDIM)

### Elucidated diffusion models (EDM)

### Flow matching and rectified flow

### Learning from feedback and reward models

### One-shot and few-shot learning

### Training-free methods

### Token learning

## Guidance

### Classifier guidance

### Classifier-free guidance

## Diffusion model frameworks

### Pixel diffusion and latent diffusion

### Optical-flow-based diffusion models

### Noise scheduling

### Agent-based diffusion models

## Architecture

### UNet

### Diffusion transformers

### VAE for latent space compression

### Text encoder

# Implementation

## Datasets

## Training engineering

## Evaluation metrics and benchmarking findings

## Industry solutions

# Applications

## Conditions

### Image condition


| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control](https://openreview.net/pdf?id=TX0OsLcaWf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.13830) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://dreamvideo2.github.io/) | ICLR 2025 |
| [ID-Animator: Zero-Shot Identity-Preserving Human Video Generation](https://arxiv.org/abs/2404.15275) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.15275) | [![Star](https://img.shields.io/github/stars/ID-Animator/ID-Animator.svg?style=social&label=Star)](https://github.com/ID-Animator/ID-Animator) | [![Website](https://img.shields.io/badge/Website-9cf)](https://id-animator.github.io/) | - |
| [I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models](https://arxiv.org/abs/2311.04145) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.04145) | [![Star](https://img.shields.io/github/stars/ali-vilab/VGen.svg?style=social&label=Star)](https://github.com/ali-vilab/VGen) | [![Website](https://img.shields.io/badge/Website-9cf)](https://i2vgen-xl.github.io/) | - |
| [DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06298.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2310.12190) | [![Star](https://img.shields.io/github/stars/Doubiiu/DynamiCrafter.svg?style=social&label=Star)](https://github.com/Doubiiu/DynamiCrafter) | [![Website](https://img.shields.io/badge/Website-9cf)](https://doubiiu.github.io/projects/DynamiCrafter/) | ECCV 2024 |
| [VideoComposer: Compositional Video Synthesis with Motion Controllability](https://proceedings.neurips.cc/paper_files/paper/2023/file/180f6184a3458fa19c28c5483bc61877-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.02018) | [![Star](https://img.shields.io/github/stars/ali-vilab/videocomposer.svg?style=social&label=Star)](https://github.com/ali-vilab/videocomposer) | [![Website](https://img.shields.io/badge/Website-9cf)](https://videocomposer.github.io/) | NIPS 2023 |
| [CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation](https://arxiv.org/abs/2406.02509) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.02509) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://ir1d.github.io/CamCo/) | - |
| [VideoBooth: Diffusion-based Video Generation with Image Prompts](https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_VideoBooth_Diffusion-based_Video_Generation_with_Image_Prompts_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.00777) | [![Star](https://img.shields.io/github/stars/Vchitect/VideoBooth.svg?style=social&label=Star)](https://github.com/Vchitect/VideoBooth) | [![Website](https://img.shields.io/badge/Website-9cf)](https://vchitect.github.io/VideoBooth-project/) | CVPR 2024 |
| [MagDiff: Multi-alignment Diffusion for High-Fidelity Video Generation and Editing](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02738.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.17338) | [![Star](https://img.shields.io/github/stars/gulucaptain/videoassembler?tab=readme-ov-file.svg?style=social&label=Star)](https://github.com/gulucaptain/videoassembler?tab=readme-ov-file) | [![Website](https://img.shields.io/badge/Website-9cf)](https://github.com/gulucaptain/videoassembler) | ECCV 2024 |
| [Stable video diffusion: Scaling latent video diffusion models to large datasets](https://arxiv.org/abs/2311.15127) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.15127) | [![Star](https://img.shields.io/github/stars/Stability-AI/generative-models.svg?style=social&label=Star)](https://github.com/Stability-AI/generative-models) | [![Website](https://img.shields.io/badge/Website-9cf)](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets) | - |
| [VDT: General-purpose Video Diffusion Transformers via Mask Modeling](https://openreview.net/pdf?id=Un0rgm9f04) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.13311) | [![Star](https://img.shields.io/github/stars/RERV/VDT.svg?style=social&label=Star)](https://github.com/RERV/VDT) | [![Website](https://img.shields.io/badge/Website-9cf)](https://vdt-2023.github.io/) | ICLR 2024 |
| [Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling](https://dl.acm.org/doi/pdf/10.1145/3641519.3657497) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2401.15977) | [![Star](https://img.shields.io/github/stars/G-U-N/Motion-I2V.svg?style=social&label=Star)](https://github.com/G-U-N/Motion-I2V) | [![Website](https://img.shields.io/badge/Website-9cf)](https://xiaoyushi97.github.io/Motion-I2V/) | SIGGRAPH 2024 |
| [AtomoVideo: High Fidelity Image-to-Video Generation](https://arxiv.org/abs/2403.01800) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.01800) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://atomo-video.github.io/) | - |
| [CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer](https://openreview.net/pdf?id=LQzN6TRFg9) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2408.06072) | [![Star](https://img.shields.io/github/stars/THUDM/CogVideo.svg?style=social&label=Star)](https://github.com/THUDM/CogVideo) | [![Website](https://img.shields.io/badge/Website-9cf)](https://github.com/THUDM/CogVideo) | ICLR 2025 |
| [Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models](https://arxiv.org/abs/2407.15642) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.15642) | [![Star](https://img.shields.io/github/stars/maxin-cn/Cinemo.svg?style=social&label=Star)](https://github.com/maxin-cn/Cinemo) | [![Website](https://img.shields.io/badge/Website-9cf)](https://maxin-cn.github.io/cinemo_project/) | CVPR 2025 |
| [TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_TRIP_Temporal_Residual_Learning_with_Image_Noise_Prior_for_Image-to-Video_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.17005) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://trip-i2v.github.io/TRIP/) | CVPR 2024 |
| [Seer: Language Instructed Video Prediction with Latent Diffusion Models](https://openreview.net/pdf?id=qHGgNyQk31) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.14897) | [![Star](https://img.shields.io/github/stars/seervideodiffusion/SeerVideoLDM.svg?style=social&label=Star)](https://github.com/seervideodiffusion/SeerVideoLDM) | [![Website](https://img.shields.io/badge/Website-9cf)](https://seervideodiffusion.github.io/) | ICLR 2024 |
| [AnimateAnything: Fine-Grained Open Domain Image Animation with Motion Guidance](https://arxiv.org/abs/2311.12886) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.12886) | [![Star](https://img.shields.io/github/stars/alibaba/animate-anything.svg?style=social&label=Star)](https://github.com/alibaba/animate-anything) | [![Website](https://img.shields.io/badge/Website-9cf)](https://animationai.github.io/AnimateAnything/) | - |
| [Make pixels dance: High-dynamic video generation](https://openaccess.thecvf.com/content/CVPR2024/html/Zeng_Make_Pixels_Dance_High-Dynamic_Video_Generation_CVPR_2024_paper.html) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.10982) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://makepixelsdance.github.io/) | CVPR 2024 |
| [MagDiff: Multi-alignment Diffusion for High-Fidelity Video Generation and Editing](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02738.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.17338) | [![Star](https://img.shields.io/github/stars/gulucaptain/videoassembler?tab=readme-ov-file.svg?style=social&label=Star)](https://github.com/gulucaptain/videoassembler?tab=readme-ov-file) | [![Website](https://img.shields.io/badge/Website-9cf)](https://github.com/gulucaptain/videoassembler) | ECCV 2024 |
| [PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_PIA_Your_Personalized_Image_Animator_via_Plug-and-Play_Modules_in_Text-to-Image_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.13964) | [![Star](https://img.shields.io/github/stars/open-mmlab/PIA.svg?style=social&label=Star)](https://github.com/open-mmlab/PIA) | [![Website](https://img.shields.io/badge/Website-9cf)](https://pi-animator.github.io/) | CVPR 2024 |
| [Adding Conditional Control to Text-to-Image Diffusion Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2302.05543) | [![Star](https://img.shields.io/github/stars/lllyasviel/ControlNet.svg?style=social&label=Star)](https://github.com/lllyasviel/ControlNet) | [![Website](https://img.shields.io/badge/Website-9cf)](https://github.com/lllyasviel/ControlNet) | ICCV 2023 |
| [DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance](https://ieeexplore.ieee.org/document/10887583) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.03018) | [![Star](https://img.shields.io/github/stars/anonymous0769/DreamVideo.svg?style=social&label=Star)](https://github.com/anonymous0769/DreamVideo) | [![Website](https://img.shields.io/badge/Website-9cf)](https://anonymous0769.github.io/DreamVideo/) | ICASSP 2025 |
| [Sparsectrl: Adding sparse controls to text-to-video diffusion models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05885.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.16933) | [![Star](https://img.shields.io/github/stars/guoyww/AnimateDiff#202312-animatediff-v3-and-sparsectrl.svg?style=social&label=Star)](https://github.com/guoyww/AnimateDiff#202312-animatediff-v3-and-sparsectrl) | [![Website](https://img.shields.io/badge/Website-9cf)](https://guoyww.github.io/projects/SparseCtrl/) | ECCV 2024 |
| [I2V-Adapter: A General Image-to-Video Adapter for Diffusion Models](https://dl.acm.org/doi/10.1145/3641519.3657407) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.16693) | [![Star](https://img.shields.io/github/stars/I2V-Adapter/I2V-Adapter-repo.svg?style=social&label=Star)](https://github.com/I2V-Adapter/I2V-Adapter-repo) | [![Website](https://img.shields.io/badge/Website-9cf)](https://i2v-adapter.github.io/) | SIGGRAPH 2024 |
| [EMO: Emote Portrait Alive-Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11028.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.17485) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://humanaigc.github.io/emote-portrait-alive/) | ECCV 2024 |
| [Videocrafter1: Open diffusion models for high-quality video generation](https://arxiv.org/abs/2310.19512) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2310.19512) | [![Star](https://img.shields.io/github/stars/AILab-CVC/VideoCrafter.svg?style=social&label=Star)](https://github.com/AILab-CVC/VideoCrafter) | [![Website](https://img.shields.io/badge/Website-9cf)](https://ailab-cvc.github.io/videocrafter2/) | - |
| [Conditional Image-to-Video Generation with Latent Flow Diffusion Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_Conditional_Image-to-Video_Generation_With_Latent_Flow_Diffusion_Models_CVPR_2023_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.13744) | [![Star](https://img.shields.io/github/stars/nihaomiao/CVPR23_LFDM.svg?style=social&label=Star)](https://github.com/nihaomiao/CVPR23_LFDM) | [![Website](https://img.shields.io/badge/Website-9cf)](https://github.com/nihaomiao/CVPR23_LFDM) | CVPR 2023 |
| [Generative Image Dynamics](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Generative_Image_Dynamics_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2309.07906) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://generative-dynamics.github.io/) | CVPR 2024 |
| [ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation](https://openreview.net/pdf?id=vqniLmUDvj) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.04324) | [![Star](https://img.shields.io/github/stars/TIGER-AI-Lab/ConsistI2V.svg?style=social&label=Star)](https://github.com/TIGER-AI-Lab/ConsistI2V) | [![Website](https://img.shields.io/badge/Website-9cf)](https://tiger-ai-lab.github.io/ConsistI2V/) | TMLR |

### Spatial condition
| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control](https://arxiv.org/abs/2407.21705) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.21705) | [![Star](https://img.shields.io/github/stars/alibaba/Tora.svg?style=social&label=Star)](https://github.com/alibaba/Tora) | [![Website](https://img.shields.io/badge/Website-9cf)](https://ali-videoai.github.io/tora_video/) | CVPR 2025 |
| [Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling](https://arxiv.org/abs/2401.15977) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2401.15977) | [![Star](https://img.shields.io/github/stars/G-U-N/Motion-I2V.svg?style=social&label=Star)](https://github.com/G-U-N/Motion-I2V) | [![Website](https://img.shields.io/badge/Website-9cf)](https://xiaoyushi97.github.io/Motion-I2V/) | SIGGRAPH 2024 |
| [FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models](https://arxiv.org/abs/2406.16863) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.16863) | [![Star](https://img.shields.io/github/stars/arthur-qiu/FreeTraj.svg?style=social&label=Star)](https://github.com/arthur-qiu/FreeTraj) | [![Website](https://img.shields.io/badge/Website-9cf)](http://haonanqiu.com/projects/FreeTraj.html) | - |
| [ObjCtrl-2.5D: Training-free Object Control with Camera Poses](https://arxiv.org/abs/2412.07721) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.07721) | [![Star](https://img.shields.io/github/stars/wzhouxiff/ObjCtrl-2.5D.svg?style=social&label=Star)](https://github.com/wzhouxiff/ObjCtrl-2.5D) | [![Website](https://img.shields.io/badge/Website-9cf)](https://wzhouxiff.github.io/projects/ObjCtrl-2.5D/) | - |
| [Boximator: Generating Rich and Controllable Motions for Video Synthesis](https://arxiv.org/abs/2402.01566) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.01566) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://boximator.github.io/) | - |
| [SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation](https://arxiv.org/abs/2411.04989) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.04989) | [![Star](https://img.shields.io/github/stars/Kmcode1/SG-I2V.svg?style=social&label=Star)](https://github.com/Kmcode1/SG-I2V) | [![Website](https://img.shields.io/badge/Website-9cf)](https://kmcode1.github.io/Projects/SG-I2V/) | ICLR 2025 |
| [MotionBooth: Motion-Aware Customized Text-to-Video Generation](https://arxiv.org/abs/2406.17758) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.17758) | [![Star](https://img.shields.io/github/stars/jianzongwu/MotionBooth.svg?style=social&label=Star)](https://github.com/jianzongwu/MotionBooth) | [![Website](https://img.shields.io/badge/Website-9cf)](https://jianzongwu.github.io/projects/motionbooth/) | NeurIPS 2024 |
| [Streetscapes: Large-scale Consistent Street View Generation Using Autoregressive Video Diffusion](http://arxiv.org/abs/2407.13759) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](http://arxiv.org/abs/2407.13759) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://boyangdeng.com/streetscapes/) | SIGGRAPH 2024 |
| [SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models](https://arxiv.org/abs/2311.16933) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.16933) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://guoyww.github.io/projects/SparseCtrl/) | ECCV 2024 |
| [CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation](https://arxiv.org/abs/2502.08639) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2502.08639) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://cinemaster-dev.github.io/) | SIGGRAPH 2025 |
| [MVideo: Motion Control for Enhanced Complex Action Video Generation](https://arxiv.org/abs/2411.08328) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.08328) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://mvideo-v1.github.io/) | - |
| [DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control](https://arxiv.org/abs/2410.13830) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.13830) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://dreamvideo2.github.io/) | - |
| [Motion Prompting: Controlling Video Generation with Motion Trajectories](https://arxiv.org/pdf/2412.02700) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/pdf/2412.02700) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://motion-prompting.github.io/) | CVPR 2025 |
| [PEEKABOO: Interactive Video Generation via Masked-Diffusion](https://arxiv.org/abs/2312.07509) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.07509) | [![Star](https://img.shields.io/github/stars/microsoft/Peekaboo.svg?style=social&label=Star)](https://github.com/microsoft/Peekaboo) | [![Website](https://img.shields.io/badge/Website-9cf)](https://yash-jain.com/projects/Peekaboo/) | CVPR 2024 |
| [DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory](https://arxiv.org/abs/2308.08089) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2308.08089) | [![Star](https://img.shields.io/github/stars/chaojie/ComfyUI-DragNUWA.svg?style=social&label=Star)](https://github.com/chaojie/ComfyUI-DragNUWA) | [![Website](https://img.shields.io/badge/Website-9cf)](https://www.microsoft.com/en-us/research/project/dragnuwa/) | - |
| [DragAnything: Motion Control for Anything using Entity Representation](https://arxiv.org/abs/2403.07420) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.07420) | [![Star](https://img.shields.io/github/stars/showlab/DragAnything.svg?style=social&label=Star)](https://github.com/showlab/DragAnything) | [![Website](https://img.shields.io/badge/Website-9cf)](https://weijiawu.github.io/draganything_page/) | ECCV 2024 |

### Camera parameter condition
| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [CameraCtrl: Enabling Camera Control for Text-to-Video Generation](https://arxiv.org/abs/2404.02101) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.02101) | [![Star](https://img.shields.io/github/stars/hehao13/CameraCtrl.svg?style=social&label=Star)](https://github.com/hehao13/CameraCtrl) | [![Website](https://img.shields.io/badge/Website-9cf)](https://hehao13.github.io/projects-CameraCtrl/) | - |
| [Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control](https://arxiv.org/abs/2405.17414) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.17414) | [![Star](https://img.shields.io/github/stars/CollaborativeVideoDiffusion/CVD.svg?style=social&label=Star)](https://github.com/CollaborativeVideoDiffusion/CVD) | [![Website](https://img.shields.io/badge/Website-9cf)](https://collaborativevideodiffusion.github.io/) | NeurIPS 2024 |
| [CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation](https://arxiv.org/abs/2406.02509) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.02509) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://ir1d.github.io/CamCo/) | - |
| [Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention](https://arxiv.org/abs/2410.10774) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.10774) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://ir1d.github.io/Cavia/) | - |
| [VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control](https://arxiv.org/abs/2407.12781) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.12781) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://snap-research.github.io/vd3d/) | ICLR 2025 |
| [AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers](https://arxiv.org/abs/2411.18673) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.18673) | [![Star](https://img.shields.io/github/stars/ac3d/.svg?style=social&label=Star)](https://github.com/snap-research/ac3d/) | [![Website](https://img.shields.io/badge/Website-9cf)](https://snap-research.github.io/ac3d/) | CVPR 2025 |
| [MotionBooth: Motion-Aware Customized Text-to-Video Generation](https://arxiv.org/abs/2406.17758) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.17758) | [![Star](https://img.shields.io/github/stars/jianzongwu/MotionBooth.svg?style=social&label=Star)](https://github.com/jianzongwu/MotionBooth) | [![Website](https://img.shields.io/badge/Website-9cf)](https://jianzongwu.github.io/projects/motionbooth/) | NeurIPS 2024 |
| [Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion](https://arxiv.org/abs/2402.03162) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.03162) | [![Star](https://img.shields.io/github/stars/ysy31415/direct_a_video.svg?style=social&label=Star)](https://github.com/ysy31415/direct_a_video) | [![Website](https://img.shields.io/badge/Website-9cf)](https://direct-a-video.github.io/) | - |
| [MotionCtrl: A Unified and Flexible Motion Controller for Video Generation](https://arxiv.org/abs/2312.03641) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.03641) | [![Star](https://img.shields.io/github/stars/TencentARC/MotionCtrl.svg?style=social&label=Star)](https://github.com/TencentARC/MotionCtrl) | [![Website](https://img.shields.io/badge/Website-9cf)](https://wzhouxiff.github.io/projects/MotionCtrl/) | SIGGRAPH 2024 |
| [3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation](https://arxiv.org/abs/2412.07759) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.07759) | [![Star](https://img.shields.io/github/stars/KwaiVGI/3DTrajMaster.svg?style=social&label=Star)](https://github.com/KwaiVGI/3DTrajMaster) | [![Website](https://img.shields.io/badge/Website-9cf)](https://fuxiao0719.github.io/projects/3dtrajmaster/) | ICLR 2025 |
| [CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation](https://arxiv.org/abs/2502.08639) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2502.08639) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://cinemaster-dev.github.io/) | SIGGRAPH 2025 |

### Audio condition
| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions](https://arxiv.org/abs/2402.17485) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.17485) | [![Star](https://img.shields.io/github/stars/HumanAIGC/EMO.svg?style=social&label=Star)](https://github.com/HumanAIGC/EMO) | [![Website](https://img.shields.io/badge/Website-9cf)](https://humanaigc.github.io/emote-portrait-alive/) | ECCV 2024 |
| [VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time](https://arxiv.org/abs/2404.10667) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.10667) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://www.microsoft.com/en-us/research/project/vasa-1/) | NeurIPS 2024 |
| [Speech2Lip: High-fidelity Speech to Lip Generation by Learning from a Short Video](https://arxiv.org/abs/2309.04814) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2309.04814) | [![Star](https://img.shields.io/github/stars/CVMI-Lab/Speech2Lip.svg?style=social&label=Star)](https://github.com/CVMI-Lab/Speech2Lip) | [![Website](https://img.shields.io/badge/Website-9cf)](https://wxzwxzwxz.github.io/Speech2Lip/) | ICCV 2023 |
| [Audio-Driven Co-Speech Gesture Video Generation](https://arxiv.org/abs/2212.02350) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.02350) | [![Star](https://img.shields.io/github/stars/alvinliu0/ANGIE.svg?style=social&label=Star)](https://github.com/alvinliu0/ANGIE) | [![Website](https://img.shields.io/badge/Website-9cf)](https://alvinliu0.github.io/projects/ANGIE) | NeurIPS 2022 |
| [OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models](https://arxiv.org/abs/2502.01061) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2502.01061) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://omnihuman-lab.github.io/) | - |
| [EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars](https://arxiv.org/abs/2404.19110) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.19110) | [![Star](https://img.shields.io/github/stars/neeek2303/EMOPortraits.svg?style=social&label=Star)](https://github.com/neeek2303/EMOPortraits) | [![Website](https://img.shields.io/badge/Website-9cf)](https://neeek2303.github.io/EMOPortraits/) | CVPR 2024 |
| [FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization](https://arxiv.org/abs/2403.06375) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.06375) | - | - | CVPR 2024 |
| [AniTalker: Animate Vivid and Diverse Talking Faces through Identity-Decoupled Facial Motion Encoding](https://arxiv.org/abs/2405.03121) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.03121) | [![Star](https://img.shields.io/github/stars/X-LANCE/AniTalker.svg?style=social&label=Star)](https://github.com/X-LANCE/AniTalker) | [![Website](https://img.shields.io/badge/Website-9cf)](https://x-lance.github.io/AniTalker/) | ACM MM 2024 |
| [AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation](https://arxiv.org/abs/2403.17694) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.17694) | [![Star](https://img.shields.io/github/stars/Zejun-Yang/AniPortrait.svg?style=social&label=Star)](https://github.com/Zejun-Yang/AniPortrait) | - | - |
| [CCVS: Context-aware Controllable Video Synthesis](https://arxiv.org/abs/2107.08037) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2107.08037) | [![Star](https://img.shields.io/github/stars/16lemoing/ccvs.svg?style=social&label=Star)](https://github.com/16lemoing/ccvs) | [![Website](https://img.shields.io/badge/Website-9cf)](https://16lemoing.github.io/ccvs/) | NeurIPS 2021 |

### High-level video condition
| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation](https://arxiv.org/abs/2212.11565) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.11565) | [![Star](https://img.shields.io/github/stars/showlab/Tune-A-Video.svg?style=social&label=Star)](https://github.com/showlab/Tune-A-Video) | [![Website](https://img.shields.io/badge/Website-9cf)](https://tuneavideo.github.io/) | ICCV 2023 |
| [Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models](https://arxiv.org/abs/2303.17599) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.17599) | [![Star](https://img.shields.io/github/stars/baaivision/vid2vid-zero.svg?style=social&label=Star)](https://github.com/baaivision/vid2vid-zero) | - | IEEE Trans On Multimedia, 2023 |
| [Towards Consistent Video Editing with Text-to-Image Diffusion Models](https://arxiv.org/abs/2305.17431) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.17431) | - | - | NeurIPS 2023 |
| [SimDA: Simple Diffusion Adapter for Efficient Video Generation](https://arxiv.org/abs/2308.09710) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2308.09710) | [![Star](https://img.shields.io/github/stars/ChenHsing/SimDA.svg?style=social&label=Star)](https://github.com/ChenHsing/SimDA) | [![Website](https://img.shields.io/badge/Website-9cf)](https://chenhsing.github.io/SimDA/) | CVPR 2024 |
| [UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing](https://arxiv.org/abs/2402.13185) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.13185) | [![Star](https://img.shields.io/github/stars/JianhongBai/UniEdit.svg?style=social&label=Star)](https://github.com/JianhongBai/UniEdit) | [![Website](https://img.shields.io/badge/Website-9cf)](https://jianhongbai.github.io/UniEdit/) | - |
| [AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks](https://arxiv.org/abs/2403.14468) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.14468) | [![Star](https://img.shields.io/github/stars/TIGER-AI-Lab/AnyV2V.svg?style=social&label=Star)](https://github.com/TIGER-AI-Lab/AnyV2V) | [![Website](https://img.shields.io/badge/Website-9cf)](https://tiger-ai-lab.github.io/AnyV2V/) | TMLR 2024 |
| [VidToMe: Video Token Merging for Zero-Shot Video Editing](https://arxiv.org/abs/2312.10656) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.10656) | [![Star](https://img.shields.io/github/stars/lixirui142/VidToMe.svg?style=social&label=Star)](https://github.com/lixirui142/VidToMe) | [![Website](https://img.shields.io/badge/Website-9cf)](https://vidtome-diffusion.github.io/) | CVPR 2024 |
| [TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation](https://arxiv.org/abs/2412.03069) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.03069) | [![Star](https://img.shields.io/github/stars/omerbt/TokenFlow.svg?style=social&label=Star)](https://github.com/omerbt/TokenFlow) | [![Website](https://img.shields.io/badge/Website-9cf)](https://diffusion-tokenflow.github.io/) | ICLR 2024 |
| [Structure and Content-Guided Video Synthesis with Diffusion Models](https://arxiv.org/abs/2302.03011) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2302.03011) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://runwayml.com/research/gen-1) | ICCV 2023 |
| [MagicEdit: High-Fidelity and Temporally Coherent Video Editing](https://arxiv.org/abs/2308.14749) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2308.14749) | [![Star](https://img.shields.io/github/stars/magic-research/magic-edit.svg?style=social&label=Star)](https://github.com/magic-research/magic-edit) | [![Website](https://img.shields.io/badge/Website-9cf)](https://magic-edit.github.io/) | - |
| [VideoComposer: Compositional Video Synthesis with Motion Controllability](https://arxiv.org/abs/2306.02018) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.02018) | [![Star](https://img.shields.io/github/stars/ali-vilab/videocomposer.svg?style=social&label=Star)](https://github.com/ali-vilab/videocomposer) | [![Website](https://img.shields.io/badge/Website-9cf)](https://videocomposer.github.io/) | NeurIPS 2023 |
| [Motion-Conditioned Image Animation for Video Editing](https://arxiv.org/abs/2311.18827) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.18827) | [![Star](https://img.shields.io/github/stars/facebookresearch/MoCA.svg?style=social&label=Star)](https://github.com/facebookresearch/MoCA) | [![Website](https://img.shields.io/badge/Website-9cf)](https://facebookresearch.github.io/MoCA/) | - |
| [FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis](https://arxiv.org/abs/2312.17681) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.17681) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://jeff-liangf.github.io/projects/flowvid/) | CVPR 2024 |
| [VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet](https://arxiv.org/abs/2307.14073) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2307.14073) | [![Star](https://img.shields.io/github/stars/ZhihaoHu/VideoControlNet.svg?style=social&label=Star)](https://github.com/ZhihaoHu/VideoControlNet) | [![Website](https://img.shields.io/badge/Website-9cf)](https://vcg-aigc.github.io/) | - |
| [MotionClone: Training-Free Motion Cloning for Controllable Video Generation](https://arxiv.org/abs/2406.05338) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.05338) | [![Star](https://img.shields.io/github/stars/LPengYang/MotionClone.svg?style=social&label=Star)](https://github.com/LPengYang/MotionClone) | [![Website](https://img.shields.io/badge/Website-9cf)](https://bujiazi.github.io/motionclone.github.io/) | ICLR 2025 |
| [MotionDirector: Motion Customization of Text-to-Video Diffusion Models](https://arxiv.org/abs/2310.08465) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2310.08465) | [![Star](https://img.shields.io/github/stars/showlab/MotionDirector.svg?style=social&label=Star)](https://github.com/showlab/MotionDirector) | [![Website](https://img.shields.io/badge/Website-9cf)](https://showlab.github.io/MotionDirector/) | ECCV 2024 |
| [SAVE: Protagonist Diversification with Structure Agnostic Video Editing](https://arxiv.org/abs/2312.02503) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.02503) | [![Star](https://img.shields.io/github/stars/ldynx/SAVE.svg?style=social&label=Star)](https://github.com/ldynx/SAVE) | [![Website](https://img.shields.io/badge/Website-9cf)](https://ldynx.github.io/SAVE/) | ECCV 2024 |
| [Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models](https://arxiv.org/abs/2305.19193) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.19193) | - | - | - |
| [Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators](https://arxiv.org/abs/2303.13439) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.13439) | [![Star](https://img.shields.io/github/stars/Picsart-AI-Research/Text2Video-Zero.svg?style=social&label=Star)](https://github.com/Picsart-AI-Research/Text2Video-Zero) | [![Website](https://img.shields.io/badge/Website-9cf)](https://text2video-zero.github.io/) | ICCV 2023 |
| [Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation](https://arxiv.org/abs/2306.07954) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.07954) | [![Star](https://img.shields.io/github/stars/williamyang1991/Rerender_A_Video.svg?style=social&label=Star)](https://github.com/williamyang1991/Rerender_A_Video) | [![Website](https://img.shields.io/badge/Website-9cf)](https://www.mmlab-ntu.com/project/rerender/) | SIGGRAPH Asia 2023 |
| [Pix2Video: Video Editing using Image Diffusion](https://arxiv.org/abs/2303.12688) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.12688) | [![Star](https://img.shields.io/github/stars/duyguceylan/pix2video.svg?style=social&label=Star)](https://github.com/duyguceylan/pix2video) | [![Website](https://img.shields.io/badge/Website-9cf)](https://duyguceylan.github.io/pix2video.github.io/) | ICCV 2023 |
| [I2VEdit: First-Frame-Guided Video Editing via Image-to-Video Diffusion Models](https://arxiv.org/abs/2405.16537) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.16537) | [![Star](https://img.shields.io/github/stars/Vicky0522/I2VEdit.svg?style=social&label=Star)](https://github.com/Vicky0522/I2VEdit) | [![Website](https://img.shields.io/badge/Website-9cf)](https://i2vedit.github.io/) | SIGGRAPH Asia 2024 |
| [RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models](https://arxiv.org/abs/2312.04524) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.04524) | [![Star](https://img.shields.io/github/stars/RehgLab/RAVE.svg?style=social&label=Star)](https://github.com/RehgLab/RAVE) | [![Website](https://img.shields.io/badge/Website-9cf)](https://rave-video.github.io/) | CVPR 2024 |
| [ControlVideo: Training-free Controllable Text-to-Video Generation](https://arxiv.org/abs/2305.13077) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.13077) | [![Star](https://img.shields.io/github/stars/YBYBZhang/ControlVideo.svg?style=social&label=Star)](https://github.com/YBYBZhang/ControlVideo) | [![Website](https://img.shields.io/badge/Website-9cf)](https://controlvideov1.github.io/) | ICLR 2024 |
| [Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding](https://arxiv.org/abs/2212.02802) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.02802) | [![Star](https://img.shields.io/github/stars/gmkim-ai/Diffusion-Video-Autoencoders.svg?style=social&label=Star)](https://github.com/gmkim-ai/Diffusion-Video-Autoencoders) | [![Website](https://img.shields.io/badge/Website-9cf)](https://diff-video-ae.github.io/) | CVPR 2023 |
| [Video Colorization with Pre-trained Text-to-Image Diffusion Models](https://arxiv.org/abs/2306.01732) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.01732) | [![Star](https://img.shields.io/github/stars/ColorDiffuser/ColorDiffuser.svg?style=social&label=Star)](https://github.com/ColorDiffuser/ColorDiffuser) | [![Website](https://img.shields.io/badge/Website-9cf)](https://colordiffuser.github.io/) | - |
| [Layered Neural Atlases for Consistent Video Editing](https://arxiv.org/abs/2109.11418) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2109.11418) | [![Star](https://img.shields.io/github/stars/ykasten/layered-neural-atlases.svg?style=social&label=Star)](https://github.com/ykasten/layered-neural-atlases) | [![Website](https://img.shields.io/badge/Website-9cf)](https://layered-neural-atlases.github.io/) | SIGGRAPH Asia 2021 |
| [VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing](https://arxiv.org/abs/2306.08707) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.08707) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://videdit.github.io/) | TMLR 2024 |
| [DiffusionAtlas: High-Fidelity Consistent Diffusion Video Editing](https://arxiv.org/abs/2312.03772) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.03772) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://diffusionatlas.github.io/) | - |
| [CoDeF: Content Deformation Fields for Temporally Consistent Video Processing](https://arxiv.org/abs/2308.07926) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2308.07926) | [![Star](https://img.shields.io/github/stars/qiuyu96/CoDeF.svg?style=social&label=Star)](https://github.com/qiuyu96/CoDeF) | [![Website](https://img.shields.io/badge/Website-9cf)](https://qiuyu96.github.io/CoDeF/) | CVPR 2024 |
| [StableVideo: Text-driven Consistency-aware Diffusion Video Editing](https://arxiv.org/abs/2308.09592) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2308.09592) | [![Star](https://img.shields.io/github/stars/rese1f/StableVideo.svg?style=social&label=Star)](https://github.com/rese1f/StableVideo) | [![Website](https://img.shields.io/badge/Website-9cf)](https://wenhaochai.com/StableVideo/) | ICCV 2023 |
| [Shape-aware Text-driven Layered Video Editing](https://arxiv.org/abs/2301.13173) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2301.13173) | [![Star](https://img.shields.io/github/stars/text-video-edit/shape-aware-text-driven-layered-video-editing-release.svg?style=social&label=Star)](https://github.com/text-video-edit/shape-aware-text-driven-layered-video-editing-release) | [![Website](https://img.shields.io/badge/Website-9cf)](https://text-video-edit.github.io/) | CVPR 2023 |
| [DragVideo: Interactive Drag-style Video Editing](https://arxiv.org/abs/2312.02216) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.02216) | [![Star](https://img.shields.io/github/stars/RickySkywalker/DragVideo-Official.svg?style=social&label=Star)](https://github.com/RickySkywalker/DragVideo-Official) | [![Website](https://img.shields.io/badge/Website-9cf)](https://dragvideo.github.io/) | ECCV 2024 |
| [Drag-A-Video: Non-rigid Video Editing with Point-based Interaction](https://arxiv.org/abs/2312.02936) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.02936) | [![Star](https://img.shields.io/github/stars/tyshiwo1/drag-a-video.svg?style=social&label=Star)](https://github.com/tyshiwo1/drag-a-video) | [![Website](https://img.shields.io/badge/Website-9cf)](https://drag-a-video.github.io/) | - |
| [ReVideo: Remake a Video with Motion and Content Control](https://arxiv.org/abs/2405.13865) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.13865) | [![Star](https://img.shields.io/github/stars/MC-E/ReVideo.svg?style=social&label=Star)](https://github.com/MC-E/ReVideo) | [![Website](https://img.shields.io/badge/Website-9cf)](https://mc-e.github.io/project/ReVideo/) | NeurIPS 2024 |
| [VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence](https://arxiv.org/abs/2312.02087) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.02087) | [![Star](https://img.shields.io/github/stars/showlab/VideoSwap.svg?style=social&label=Star)](https://github.com/showlab/VideoSwap) | [![Website](https://img.shields.io/badge/Website-9cf)](https://videoswap.github.io/) | CVPR 2024 |
| [Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos](https://arxiv.org/abs/2304.01186) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.01186) | [![Star](https://img.shields.io/github/stars/mayuelala/FollowYourPose.svg?style=social&label=Star)](https://github.com/mayuelala/FollowYourPose) | [![Website](https://img.shields.io/badge/Website-9cf)](https://follow-your-pose.github.io/) | AAAI 2024 |
| [MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-aware Diffusion](https://arxiv.org/abs/2311.12052) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.12052) | [![Star](https://img.shields.io/github/stars/Boese0601/MagicDance.svg?style=social&label=Star)](https://github.com/Boese0601/MagicDance) | [![Website](https://img.shields.io/badge/Website-9cf)](https://boese0601.github.io/magicdance/) | ICML 2024 |
| [Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation](https://arxiv.org/abs/2311.17117) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.17117) | [![Star](https://img.shields.io/github/stars/HumanAIGC/AnimateAnyone.svg?style=social&label=Star)](https://github.com/HumanAIGC/AnimateAnyone) | [![Website](https://img.shields.io/badge/Website-9cf)](https://humanaigc.github.io/animate-anyone/) | CVPR 2024 |
| [DisCo: Disentangled Control for Realistic Human Dance Generation](https://arxiv.org/abs/2307.00040) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2307.00040) | [![Star](https://img.shields.io/github/stars/Wangt-CN/DisCo.svg?style=social&label=Star)](https://github.com/Wangt-CN/DisCo) | [![Website](https://img.shields.io/badge/Website-9cf)](https://disco-dance.github.io/) | CVPR 2024 |
| [DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion](https://arxiv.org/abs/2304.06025) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.06025) | [![Star](https://img.shields.io/github/stars/johannakarras/DreamPose.svg?style=social&label=Star)](https://github.com/johannakarras/DreamPose) | [![Website](https://img.shields.io/badge/Website-9cf)](https://grail.cs.washington.edu/projects/dreampose/) | ICCV 2023 |
| [AniTalker: Animate Vivid and Diverse Talking Faces through Identity-Decoupled Facial Motion Encoding](https://arxiv.org/abs/2405.03121) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.03121) | [![Star](https://img.shields.io/github/stars/X-LANCE/AniTalker.svg?style=social&label=Star)](https://github.com/X-LANCE/AniTalker) | [![Website](https://img.shields.io/badge/Website-9cf)](https://x-lance.github.io/AniTalker/) | ACM MM 2024 |
| [DPE: Disentanglement of Pose and Expression for General Video Portrait Editing](https://arxiv.org/abs/2301.06281) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2301.06281) | [![Star](https://img.shields.io/github/stars/OpenTalker/DPE.svg?style=social&label=Star)](https://github.com/OpenTalker/DPE) | [![Website](https://img.shields.io/badge/Website-9cf)](https://carlyx.github.io/DPE/) | CVPR 2023 |
| [InstructVid2Vid: Controllable Video Editing with Natural Language Instructions](https://arxiv.org/abs/2305.12328) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.12328) | - | - | - |
| [Consistent Video-to-Video Transfer Using Synthetic Dataset](https://arxiv.org/abs/2311.00213) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.00213) | [![Star](https://img.shields.io/github/stars/amazon-science/instruct-video-to-video.svg?style=social&label=Star)](https://github.com/amazon-science/instruct-video-to-video) | [![Website](https://img.shields.io/badge/Website-9cf)](https://www.amazon.science/publications/consistent-video-to-video-transfer-using-synthetic-dataset) | ICLR 2024 |
| [A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing](https://arxiv.org/abs/2312.05856) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.05856) | [![Star](https://img.shields.io/github/stars/STEM-Inv/STEM-Inv.svg?style=social&label=Star)](https://github.com/STEM-Inv/STEM-Inv) | [![Website](https://img.shields.io/badge/Website-9cf)](https://stem-inv.github.io/page/) | CVPR 2024 |

### Other conditions
| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation](https://arxiv.org/abs/2409.18964) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2409.18964) | [![Star](https://img.shields.io/github/stars/stevenlsw/physgen.svg?style=social&label=Star)](https://github.com/stevenlsw/physgen) | [![Website](https://img.shields.io/badge/Website-9cf)](https://stevenlsw.github.io/physgen/) | ECCV 2024 |
| [MotionCraft: Physics-based Zero-Shot Video Generation](https://arxiv.org/abs/2405.13557) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.13557) | [![Star](https://img.shields.io/github/stars/cure-lab/MotionCraft.svg?style=social&label=Star)](https://github.com/cure-lab/MotionCraft) | [![Website](https://img.shields.io/badge/Website-9cf)](https://cure-lab.github.io/MotionCraft/) | AAAI 2025 |
| [VideoAgent: Long-form Video Understanding with Large Language Model as Agent](https://arxiv.org/abs/2403.10517) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.10517) | [![Star](https://img.shields.io/github/stars/YueFan1014/VideoAgent.svg?style=social&label=Star)](https://github.com/YueFan1014/VideoAgent) | [![Website](https://img.shields.io/badge/Website-9cf)](https://videoagent.github.io/) | ECCV 2024 |
| [Synthetic Generation of Face Videos with Plethysmograph Physiology](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Synthetic_Generation_of_Face_Videos_With_Plethysmograph_Physiology_CVPR_2022_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Synthetic_Generation_of_Face_Videos_With_Plethysmograph_Physiology_CVPR_2022_paper.pdf) | - | - | CVPR 2022 |
| [Mind the Time: Temporally-Controlled Multi-Event Video Generation](https://arxiv.org/abs/2412.05263) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.05263) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://mint-video.github.io/) | CVPR 2025 |

## Enhancement

### Video denoising and deblurring

### Video inpainting

### Video interpolation and extrapolation/prediction

### Video super-resolution

### Combining multiple video enhancement tasks

## Personalization
| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance](https://ieeexplore.ieee.org/document/10887583) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.03018) | [![Star](https://img.shields.io/github/stars/anonymous0769/DreamVideo.svg?style=social&label=Star)](https://github.com/anonymous0769/DreamVideo) | [![Website](https://img.shields.io/badge/Website-9cf)](https://anonymous0769.github.io/DreamVideo/) | ICASSP 2025 |
| [PersonalVideo: High ID-Fidelity Video Customization without Dynamic and Semantic Degradation](https://arxiv.org/abs/2411.17048) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.17048) | [![Star](https://img.shields.io/github/stars/EchoPluto/PersonalVideo.svg?style=social&label=Star)](https://github.com/EchoPluto/PersonalVideo) | [![Website](https://img.shields.io/badge/Website-9cf)](https://personalvideo.github.io/) | - |
| [Still-Moving: Customized Video Generation without Customized Video Data](https://arxiv.org/abs/2407.08674) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.08674) | [![Star](https://img.shields.io/github/stars/harshbhatt7585/StillMoving.svg?style=social&label=Star)](https://github.com/harshbhatt7585/StillMoving) | [![Website](https://img.shields.io/badge/Website-9cf)](https://still-moving.github.io/) | ACM TOG 2024 |
| [Dynamic Concepts Personalization from Single Videos](https://arxiv.org/abs/2502.14844) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2502.14844) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://snap-research.github.io/dynamic_concepts/) | SIGGRAPH 2025 |
| [VideoAlchemy: Open-set Personalization in Video Generation](https://arxiv.org/abs/2501.06187) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2501.06187) | [![Star](https://img.shields.io/github/stars/snap-research/MSRVTT-Personalization.svg?style=social&label=Star)](https://github.com/snap-research/MSRVTT-Personalization) | [![Website](https://img.shields.io/badge/Website-9cf)](https://snap-research.github.io/open-set-video-personalization/) | CVPR 2025 |
| [EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11028.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.17485) | [![Star](https://img.shields.io/github/stars/HumanAIGC/EMO.svg?style=social&label=Star)](https://github.com/HumanAIGC/EMO)|[![Website](https://img.shields.io/badge/Website-9cf)](https://humanaigc.github.io/emote-portrait-alive/) | ECCV 2024 |
| [VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time](https://arxiv.org/abs/2404.10667) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.10667) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://www.microsoft.com/en-us/research/project/vasa-1/) | NeurIPS 2024 |
| [AniTalker: Animate Vivid and Diverse Talking Faces through Identity-Decoupled Facial Motion Encoding](https://arxiv.org/abs/2405.03121) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.03121) | [![Star](https://img.shields.io/github/stars/X-LANCE/AniTalker.svg?style=social&label=Star)](https://github.com/X-LANCE/AniTalker) | [![Website](https://img.shields.io/badge/Website-9cf)](https://x-lance.github.io/AniTalker/) | ACM MM 2024 |
| [FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization](https://arxiv.org/abs/2403.06375) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.06375) | - | - | CVPR 2024 |
| [Audio-Driven Co-Speech Gesture Video Generation](https://arxiv.org/abs/2212.02350) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.02350) | [![Star](https://img.shields.io/github/stars/alvinliu0/ANGIE.svg?style=social&label=Star)](https://github.com/alvinliu0/ANGIE) | [![Website](https://img.shields.io/badge/Website-9cf)](https://alvinliu0.github.io/projects/ANGIE) | NeurIPS 2022 |
| [DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control](https://openreview.net/pdf?id=TX0OsLcaWf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.13830) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://dreamvideo2.github.io/) | ICLR 2025 |
| [Identity-PreservingText-to-VideoGenerationbyFrequencyDecomposition](https://arxiv.org/abs/2411.17440) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.17440) | [![Star](https://img.shields.io/github/stars/PKU-YuanGroup/ConsisID.svg?style=social&label=Star)](https://github.com/PKU-YuanGroup/ConsisID) | [![Website](https://img.shields.io/badge/Website-9cf)](https://pku-yuangroup.github.io/ConsisID/) | CVPR 2025 |
| [MCNet: Rethinking the Core Ingredients for Accurate and Efficient Homography Estimation](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_MCNet_Rethinking_the_Core_Ingredients_for_Accurate_and_Efficient_Homography_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_MCNet_Rethinking_the_Core_Ingredients_for_Accurate_and_Efficient_Homography_CVPR_2024_paper.pdf) | [![Star](https://img.shields.io/github/stars/zjuzhk/MCNet.svg?style=social&label=Star)](https://github.com/zjuzhk/MCNet) | - | CVPR 2024 |

## Consistency

| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks](https://openreview.net/forum?id=Czsdv-S4-w9) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2202.10571) | [![Star](https://img.shields.io/github/stars/sihyun-yu/digan.svg?style=social&label=Star)](https://github.com/sihyun-yu/digan)|[![Website](https://img.shields.io/badge/Website-9cf)](https://sihyun.me/digan/) | ICLR 2022 |
| [StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2](https://openaccess.thecvf.com/content/CVPR2022/papers/Skorokhodov_StyleGAN-V_A_Continuous_Video_Generator_With_the_Price_Image_Quality_CVPR_2022_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2112.14683) | [![Star](https://img.shields.io/github/stars/universome/stylegan-v.svg?style=social&label=Star)](https://github.com/universome/stylegan-v)|[![Website](https://img.shields.io/badge/Website-9cf)](https://skor.sh/stylegan-v.html) | CVPR 2022 |
| [Towards Smooth Video Composition](https://openreview.net/forum?id=W918Ora75q) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.07413) | [![Star](https://img.shields.io/github/stars/genforce/StyleSV.svg?style=social&label=Star)](https://github.com/genforce/StyleSV)|[![Website](https://img.shields.io/badge/Website-9cf)](https://genforce.github.io/StyleSV/) | ICLR 2023 |
| [Conditional Image-to-Video Generation with Latent Flow Diffusion Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_Conditional_Image-to-Video_Generation_With_Latent_Flow_Diffusion_Models_CVPR_2023_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.13744) | [![Star](https://img.shields.io/github/stars/nihaomiao/CVPR23_LFDM.svg?style=social&label=Star)](https://github.com/nihaomiao/CVPR23_LFDM)| - | CVPR 2023 |
| [MoStGAN-V: Video Generation with Temporal Motion Styles](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_MoStGAN-V_Video_Generation_With_Temporal_Motion_Styles_CVPR_2023_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.02777) | [![Star](https://img.shields.io/github/stars/xiaoqian-shen/MoStGAN-V.svg?style=social&label=Star)](https://github.com/xiaoqian-shen/MoStGAN-V)| - | CVPR 2023 |
| [MOSO: Decomposing MOtion, Scene and Object for Video Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_MOSO_Decomposing_MOtion_Scene_and_Object_for_Video_Prediction_CVPR_2023_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.03684) | [![Star](https://img.shields.io/github/stars/iva-mzsun/MOSO.svg?style=social&label=Star)](https://github.com/iva-mzsun/MOSO)| - | CVPR 2023 |
| [Stablevideo: Text-driven consistency-aware diffusion video editing](https://openaccess.thecvf.com/content/ICCV2023/papers/Chai_StableVideo_Text-driven_Consistency-aware_Diffusion_Video_Editing_ICCV_2023_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2308.09592) |[![Star](https://img.shields.io/github/stars/rese1f/StableVideo.svg?style=social&label=Star)](https://github.com/rese1f/StableVideo)|[![HuggingFace Demo](https://img.shields.io/badge/Website-9cf)](https://huggingface.co/spaces/wchai/StableVideo) | ICCV 2023 |
| [Preserve your own correlation: A noise prior for video diffusion models](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.10474) | - |[![Website](https://img.shields.io/badge/Website-9cf)](https://research.nvidia.com/labs/dir/pyoco/) | ICCV 2023 |
| [Scenescape: Text-driven consistent scene generation](https://openreview.net/forum?id=NU2kGsA4TT) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2302.01133) | [![Star](https://img.shields.io/github/stars/RafailFridman/SceneScape.svg?style=social&label=Star)](https://github.com/RafailFridman/SceneScape)|[![Website](https://img.shields.io/badge/Website-9cf)](https://scenescape.github.io) | NeurIPS 2023 |
| [GLOBER: Coherent Non-autoregressive Video Generation via GLOBal Guided Video DecodER](https://openreview.net/forum?id=TRbklCR2ZW&referrer=%5Bthe%20profile%20of%20Jing%20Liu%5D(%2Fprofile%3Fid%3D~Jing_Liu1)) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2309.13274) | [![Star](https://img.shields.io/github/stars/iva-mzsun/GLOBER.svg?style=social&label=Star)](https://github.com/iva-mzsun/GLOBER)| - | NeurIPS 2023 |
| [VideoComposer: Compositional Video Synthesis with Motion Controllability](https://openreview.net/forum?id=h4r00NGkjR) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.02018) | [![Star](https://img.shields.io/github/stars/ali-vilab/videocomposer.svg?style=social&label=Star)](https://github.com/ali-vilab/videocomposer)|[![Website](https://img.shields.io/badge/Website-9cf)](https://videocomposer.github.io) | NeurIPS 2023 |
| [How i warped your noise: a temporally-correlated noise prior for diffusion models](https://openreview.net/forum?id=pzElnMrgSD) | - | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://warpyournoise.github.io) | ICLR 2024 |
| [Tokenflow: Consistent diffusion features for consistent video editing](https://openreview.net/forum?id=lKK50q2MtV) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2307.10373) | [![Star](https://img.shields.io/github/stars/omerbt/TokenFlow.svg?style=social&label=Star)](https://github.com/omerbt/TokenFlow)|[![Website](https://img.shields.io/badge/Website-9cf)](https://diffusion-tokenflow.github.io) | ICLR 2024 |
| [Seine: Short-to-long video diffusion model for generative transition and prediction.](https://openreview.net/forum?id=FNq3nIvP4F) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2310.20700) | [![Star](https://img.shields.io/github/stars/Vchitect/SEINE.svg?style=social&label=Star)](https://github.com/Vchitect/SEINE)|[![Website](https://img.shields.io/badge/Website-9cf)](https://vchitect.github.io/SEINE-project/) | ICLR 2024 |
| [VideoBooth: Diffusion-based Video Generation with Image Prompts](https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_VideoBooth_Diffusion-based_Video_Generation_with_Image_Prompts_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.00777) | [![Star](https://img.shields.io/github/stars/Vchitect/VideoBooth.svg?style=social&label=Star)](https://github.com/Vchitect/VideoBooth)|[![Website](https://img.shields.io/badge/Website-9cf)](https://vchitect.github.io/VideoBooth-project/) | CVPR 2024 |
| [TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Vieo Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_TRIP_Temporal_Residual_Learning_with_Image_Noise_Prior_for_Image-to-Video_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.17005) | - |[![Website](https://img.shields.io/badge/Website-9cf)](https://trip-i2v.github.io/TRIP/) | CVPR 2024 |
| [CAMEL: CAusal Motion Enhancement tailored for Lifting Text-driven Video Editing](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_CAMEL_CAusal_Motion_Enhancement_Tailored_for_Lifting_Text-driven_Video_Editing_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10655591) | [![Star](https://img.shields.io/github/stars/zhangguiwei610/CAMEL.svg?style=social&label=Star)](https://github.com/zhangguiwei610/CAMEL)| - | CVPR 2024 |
| [VidToMe: Video Token Merging for Zero-Shot Video Editing](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_VidToMe_Video_Token_Merging_for_Zero-Shot_Video_Editing_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.10656) | [![Star](https://img.shields.io/github/stars/lixirui142/VidToMe.svg?style=social&label=Star)](https://github.com/lixirui142/VidToMe)|[![Website](https://img.shields.io/badge/Website-9cf)](https://vidtome-diffusion.github.io) | CVPR 2024 |
| [EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11028.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.17485) | [![Star](https://img.shields.io/github/stars/HumanAIGC/EMO.svg?style=social&label=Star)](https://github.com/HumanAIGC/EMO)|[![Website](https://img.shields.io/badge/Website-9cf)](https://humanaigc.github.io/emote-portrait-alive/) | ECCV 2024 |
| [StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation](https://openreview.net/pdf/992e1d8483d14f713dff3f74f664f722bfa72930.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.01434) | [![Star](https://img.shields.io/github/stars/HVision-NKU/StoryDiffusion.svg?style=social&label=Star)](https://github.com/HVision-NKU/StoryDiffusion)|[![Website](https://img.shields.io/badge/Website-9cf)](https://storydiffusion.github.io) | NeurIPS 2024 |
| [Streetscapes: Large-scale consistent street view generation using autoregressive video diffusion](https://dl.acm.org/doi/10.1145/3641519.3657513) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.13759) | - |[![Website](https://img.shields.io/badge/Website-9cf)](https://boyangdeng.com/streetscapes/) | SIGGRAPH 2024 |
| [Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling](https://dl.acm.org/doi/10.1145/3641519.3657497) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2401.15977) | [![Star](https://img.shields.io/github/stars/G-U-N/Motion-I2V.svg?style=social&label=Star)](https://github.com/G-U-N/Motion-I2V)|[![Website](https://img.shields.io/badge/Website-9cf)](https://xiaoyushi97.github.io/Motion-I2V/) | SIGGRAPH 2024 |
| [Consisti2v: Enhancing visual consistency for image-to-video generation](https://openreview.net/forum?id=vqniLmUDvj) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.04324) | [![Star](https://img.shields.io/github/stars/TIGER-AI-Lab/ConsistI2V.svg?style=social&label=Star)](https://github.com/TIGER-AI-Lab/ConsistI2V)|[![Website](https://img.shields.io/badge/Website-9cf)](https://tiger-ai-lab.github.io/ConsistI2V/) | TMLR 2024 |
| [StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text](https://arxiv.org/abs/2403.14773) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.14773) | [![Star](https://img.shields.io/github/stars/Picsart-AI-Research/StreamingT2V.svg?style=social&label=Star)](https://github.com/Picsart-AI-Research/StreamingT2V)|[![Website](https://img.shields.io/badge/Website-9cf)](https://streamingt2v.github.io) | Arxiv 2024 |
| [Flexifilm: Long video generation with flexible conditions](https://arxiv.org/abs/2404.18620) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.18620) | [![Star](https://img.shields.io/github/stars/Y-ichen/FlexiFilm.svg?style=social&label=Star)](https://github.com/Y-ichen/FlexiFilm)|[![Website](https://img.shields.io/badge/Website-9cf)](https://y-ichen.github.io/FlexiFilm-Page/) | Arxiv 2024 |
| [Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models  ](https://arxiv.org/abs/2407.15642) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.15642) | [![Star](https://img.shields.io/github/stars/maxin-cn/Cinemo.svg?style=social&label=Star)](https://github.com/maxin-cn/Cinemo)|[![Website](https://img.shields.io/badge/Website-9cf)](https://maxin-cn.github.io/cinemo_project/) | CVPR 2025 |
| [AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation](https://arxiv.org/abs/2403.17694) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.17694) | [![Star](https://img.shields.io/github/stars/Zejun-Yang/AniPortrait.svg?style=social&label=Star)](https://github.com/Zejun-Yang/AniPortrait) | - | - |
| [DreamTalk: When Emotional Talking Head Generation Meets Diffusion Probabilistic Models](https://arxiv.org/abs/2312.09767) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.09767) | [![Star](https://img.shields.io/github/stars/ali-vilab/dreamtalk.svg?style=social&label=Star)](https://github.com/ali-vilab/dreamtalk) | [![Website](https://img.shields.io/badge/Website-9cf)](https://dreamtalk-project.github.io/) | - |
| [EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars](https://arxiv.org/abs/2404.19110) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.19110) | [![Star](https://img.shields.io/github/stars/neeek2303/EMOPortraits.svg?style=social&label=Star)](https://github.com/neeek2303/EMOPortraits) | [![Website](https://img.shields.io/badge/Website-9cf)](https://neeek2303.github.io/EMOPortraits/) | CVPR 2024 |

## Long video

| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2](https://openaccess.thecvf.com/content/CVPR2022/papers/Skorokhodov_StyleGAN-V_A_Continuous_Video_Generator_With_the_Price_Image_Quality_CVPR_2022_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2112.14683) | - |[![Website](https://img.shields.io/badge/Website-9cf)](https://skor.sh/stylegan-v.html) | CVPR 2022 |
| [Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136770103.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2204.03638) | [![Star](https://img.shields.io/github/stars/SongweiGe/TATS.svg?style=social&label=Star)](https://github.com/SongweiGe/TATS)|[![Website](https://img.shields.io/badge/Website-9cf)](https://songweige.github.io/projects/tats/index.html) | ECCV 2022 |
| [Generating Long Videos of Dynamic Scenes](https://openreview.net/forum?id=VnAwNNJiwDb) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2206.03429) | [![Star](https://img.shields.io/github/stars/NVlabs/long-video-gan.svg?style=social&label=Star)](https://github.com/NVlabs/long-video-gan)|[![Website](https://img.shields.io/badge/Website-9cf)](https://www.timothybrooks.com/tech/long-video-gan/) | NeurIPS 2022 |
| [Flexible Diffusion Modeling of Long Videos](https://openreview.net/forum?id=0RTJcuvHtIu) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2205.11495) | - |[![Website](https://img.shields.io/badge/Website-9cf)](https://fdmolv.github.io) | NeurIPS 2022 |
| [CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers](https://openreview.net/forum?id=rB6TpjAuSRy) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2205.15868) | [![Star](https://img.shields.io/github/stars/THUDM/CogVideo.svg?style=social&label=Star)](https://github.com/THUDM/CogVideo)| - | ICLR 2023 |
| [Towards Smooth Video Composition](https://openreview.net/forum?id=W918Ora75q) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.07413) | [![Star](https://img.shields.io/github/stars/genforce/StyleSV.svg?style=social&label=Star)](https://github.com/genforce/StyleSV)|[![Website](https://img.shields.io/badge/Website-9cf)](https://genforce.github.io/StyleSV/) | ICLR 2023 |
| [NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation](https://aclanthology.org/2023.acl-long.73/) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.12346) | - |[![Website](https://img.shields.io/badge/Website-9cf)](https://msra-nuwa.azurewebsites.net) | ACL 2023 |
| [Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising](https://arxiv.org/abs/2305.18264) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.18264) | [![Star](https://img.shields.io/github/stars/G-U-N/Gen-L-Video.svg?style=social&label=Star)](https://github.com/G-U-N/Gen-L-Video)|[![Website](https://img.shields.io/badge/Website-9cf)](https://g-u-n.github.io/projects/gen-long-video/index.html) | Arxiv 2023 |
| [FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling](https://openreview.net/pdf/bd47f35c18df619e675c737ccc56c1d802537b73.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2310.15169) | [![Star](https://img.shields.io/github/stars/AILab-CVC/FreeNoise.svg?style=social&label=Star)](https://github.com/AILab-CVC/FreeNoise)|[![Website](https://img.shields.io/badge/Website-9cf)](http://haonanqiu.com/projects/FreeNoise.html) | ICLR 2024 |
| [ViD-GPT: Introducing GPT-style Autoregressive Generation in Video Diffusion Models](https://arxiv.org/abs/2406.10981) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.10981) | [![Star](https://img.shields.io/github/stars/Dawn-LX/CausalCache-VDM.svg?style=social&label=Star)](https://github.com/Dawn-LX/CausalCache-VDM)| - | Arxiv 2024 |
| [Video-Infinity: Distributed Long Video Generation](https://arxiv.org/abs/2406.16260) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.16260) | [![Star](https://img.shields.io/github/stars/Yuanshi9815/Video-Infinity.svg?style=social&label=Star)](https://github.com/Yuanshi9815/Video-Infinity)| - | Arxiv 2024 |
| [Progressive Autoregressive Video Diffusion Models](https://arxiv.org/abs/2410.08151) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.08151) | [![Star](https://img.shields.io/github/stars/desaixie/pa_vdm.svg?style=social&label=Star)](https://github.com/desaixie/pa_vdm)|[![Website](https://img.shields.io/badge/Website-9cf)](https://desaixie.github.io/pa-vdm/) | Arxiv 2024 |
| [MAVIN: Multi-Action Video Generation with Diffusion Models via Transition Video Infilling](https://arxiv.org/abs/2405.18003) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.18003) | [![Star](https://img.shields.io/github/stars/18445864529/MAVIN.svg?style=social&label=Star)](https://github.com/18445864529/MAVIN)| - | Arxiv 2024 |
| [MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequences](https://openreview.net/forum?id=QHj2LL958o) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.16655) | [![Star](https://img.shields.io/github/stars/aim-uofa/MovieDreamer.svg?style=social&label=Star)](https://github.com/aim-uofa/MovieDreamer)|[![Website](https://img.shields.io/badge/Website-9cf)](https://aim-uofa.github.io/MovieDreamer/) | ICLR 2025 |

## 3D-aware video diffusion

### Training on 3D dataset

| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [Sora Generates Videos with Stunning Geometrical Consistency](https://arxiv.org/abs/2402.17403) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.17403) | [![Star](https://img.shields.io/github/stars/meteorshowers/Sora-Generates-Videos-with-Stunning-Geometrical-Consistency.svg?style=social&label=Star)](https://github.com/meteorshowers/Sora-Generates-Videos-with-Stunning-Geometrical-Consistency) | [![Website](https://img.shields.io/badge/Website-9cf)](https://sora-geometrical-consistency.github.io/) | - |
| [Stable video diffusion: Scaling latent video diffusion models to large datasets](https://arxiv.org/abs/2311.15127) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.15127) | [![Star](https://img.shields.io/github/stars/Stability-AI/generative-models.svg?style=social&label=Star)](https://github.com/Stability-AI/generative-models) | [![Website](https://img.shields.io/badge/Website-9cf)](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets) | - |
| [Google Scanned Objects: A High-Quality Dataset of 3D Scanned Household Items](https://ieeexplore.ieee.org/abstract/document/9811809) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2204.11918) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://research.google/blog/scanned-objects-by-google-research-a-dataset-of-3d-scanned-common-household-items/) | ICRA 2022 |
| [OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_OmniObject3D_Large-Vocabulary_3D_Object_Dataset_for_Realistic_Perception_Reconstruction_and_CVPR_2023_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2301.07525) | [![Star](https://img.shields.io/github/stars/omniobject3d/OmniObject3D.svg?style=social&label=Star)](https://github.com/omniobject3d/OmniObject3D/tree/main) | [![Website](https://img.shields.io/badge/Website-9cf)](https://omniobject3d.github.io/) | CVPR 2023 |
| [V3D: Video Diffusion Models are Effective 3D Generators](https://arxiv.org/abs/2403.06738) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.06738) | [![Star](https://img.shields.io/github/stars/heheyas/V3D.svg?style=social&label=Star)](https://github.com/heheyas/V3D) | [![Website](https://img.shields.io/badge/Website-9cf)](https://heheyas.github.io/V3D/) | - |
| [VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00255.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.12034) | [![Star](https://img.shields.io/github/stars/facebookresearch/vfusion3d.svg?style=social&label=Star)](https://github.com/facebookresearch/vfusion3d) | [![Website](https://img.shields.io/badge/Website-9cf)](https://junlinhan.github.io/projects/vfusion3d.html) | ECCV 2024 |
| [IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation](https://dl.acm.org/doi/10.5555/3692070.3693507) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.08682) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://lukemelas.github.io/IM-3D/) | ICML 2024 |
| [Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03510.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.14868) | [![Star](https://img.shields.io/github/stars/basilevh/gcd.svg?style=social&label=Star)](https://github.com/basilevh/gcd) | [![Website](https://img.shields.io/badge/Website-9cf)](https://gcd.cs.columbia.edu/) | ECCV 2024 |
| [CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation](https://arxiv.org/abs/2406.02509) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.02509) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://ir1d.github.io/CamCo/) | - |
| [Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval](https://openaccess.thecvf.com/content/ICCV2021/papers/Bain_Frozen_in_Time_A_Joint_Video_and_Image_Encoder_for_ICCV_2021_paper) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2104.00650) | [![Star](https://img.shields.io/github/stars/m-bain/webvid?tab=readme-ov-file.svg?style=social&label=Star)](https://github.com/m-bain/webvid?tab=readme-ov-file) | [![Website](https://img.shields.io/badge/Website-9cf)](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/) | ICCV 2021 |
| [Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control](https://proceedings.neurips.cc/paper_files/paper/2024/file/1d49235669869ab737c1da9d64b7c769-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.17414) | [![Star](https://img.shields.io/github/stars/CollaborativeVideoDiffusion/CVD.svg?style=social&label=Star)](https://github.com/CollaborativeVideoDiffusion/CVD) | [![Website](https://img.shields.io/badge/Website-9cf)](https://collaborativevideodiffusion.github.io/) | NeurIPS 2024 |
| [Stereo magnification: Learning view synthesis using multiplane images](https://dl.acm.org/doi/pdf/10.1145/3197517.3201323) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1805.09817) | [![Star](https://img.shields.io/github/stars/google/stereo-magnification.svg?style=social&label=Star)](https://github.com/google/stereo-magnification) | [![Website](https://img.shields.io/badge/Website-9cf)](https://tinghuiz.github.io/projects/mpi/) | SIGGRAPH 2018 |
| [Diffusion4D: Fast Spatial-temporal Consistent 4D generation via Video Diffusion Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/c7f4dbb8f3739b36029ba71a47844696-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.16645) | [![Star](https://img.shields.io/github/stars/VITA-Group/Diffusion4D.svg?style=social&label=Star)](https://github.com/VITA-Group/Diffusion4D) | [![Website](https://img.shields.io/badge/Website-9cf)](https://vita-group.github.io/Diffusion4D/) | NeurIPS 2024 |
| [Cavia: Camera-controllable multi-view video diffusion with view-integrated attention](https://arxiv.org/abs/2410.10774) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.10774) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://ir1d.github.io/Cavia/) | - |
| [RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos](https://openaccess.thecvf.com/content/CVPR2024/papers/Xia_RGBD_Objects_in_the_Wild_Scaling_Real-World_3D_Object_Learning_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2401.12592) | [![Star](https://img.shields.io/github/stars/wildrgbd/wildrgbd.svg?style=social&label=Star)](https://github.com/wildrgbd/wildrgbd) | [![Website](https://img.shields.io/badge/Website-9cf)](https://wildrgbd.github.io/) | CVPR 2024 |
| [MVImgNet: A Large-scale Dataset of Multi-view Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MVImgNet_A_Large-Scale_Dataset_of_Multi-View_Images_CVPR_2023_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.06042) | [![Star](https://img.shields.io/github/stars/GAP-LAB-CUHK-SZ/MVImgNet.svg?style=social&label=Star)](https://github.com/GAP-LAB-CUHK-SZ/MVImgNet) | [![Website](https://img.shields.io/badge/Website-9cf)](https://gaplab.cuhk.edu.cn/projects/MVImgNet/) | CVPR 2023 |
| [Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction](https://openaccess.thecvf.com/content/ICCV2021/papers/Reizenstein_Common_Objects_in_3D_Large-Scale_Learning_and_Evaluation_of_Real-Life_ICCV_2021_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2109.00512) | [![Star](https://img.shields.io/github/stars/facebookresearch/co3d.svg?style=social&label=Star)](https://github.com/facebookresearch/co3d) | [![Website](https://img.shields.io/badge/Website-9cf)](https://github.com/facebookresearch/co3d) | ICCV 2021 |
| [DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision](https://openaccess.thecvf.com/content/CVPR2024/papers/Ling_DL3DV-10K_A_Large-Scale_Scene_Dataset_for_Deep_Learning-based_3D_Vision_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.16256) | [![Star](https://img.shields.io/github/stars/DL3DV-10K/Dataset.svg?style=social&label=Star)](https://github.com/DL3DV-10K/Dataset) | [![Website](https://img.shields.io/badge/Website-9cf)](https://dl3dv-10k.github.io/DL3DV-10K/) | CVPR 2024 |
| [Objaverse: A Universe of Annotated 3D Objects](https://openaccess.thecvf.com/content/CVPR2023/papers/Deitke_Objaverse_A_Universe_of_Annotated_3D_Objects_CVPR_2023_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.08051) | [![Star](https://img.shields.io/github/stars/allenai/objaverse-xl.svg?style=social&label=Star)](https://github.com/allenai/objaverse-xl) | [![Website](https://img.shields.io/badge/Website-9cf)](https://objaverse.allenai.org/) | CVPR 2023 |
| [Objaverse-XL: A Universe of 10M+ 3D Objects](https://proceedings.neurips.cc/paper_files/paper/2023/file/70364304877b5e767de4e9a2a511be0c-Paper-Datasets_and_Benchmarks.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2307.05663) | [![Star](https://img.shields.io/github/stars/allenai/objaverse-xl.svg?style=social&label=Star)](https://github.com/allenai/objaverse-xl) | [![Website](https://img.shields.io/badge/Website-9cf)](https://objaverse.allenai.org/) | NeurIPS D&B 2023 |
| [OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation](https://openreview.net/pdf?id=j7kdXSrISM) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.02371) | [![Star](https://img.shields.io/github/stars/NJU-PCALab/OpenVid-1M.svg?style=social&label=Star)](https://github.com/NJU-PCALab/OpenVid-1M) | [![Website](https://img.shields.io/badge/Website-9cf)](https://github.com/NJU-PCALab/OpenVid-1M) | ICLR 2025 |
| [InternVid: Learning Text-to-Video Generation from Web-scale Video-Text Data](https://openreview.net/pdf?id=MLBdiWu4Fw) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2307.06942) | [![Star](https://img.shields.io/github/stars/OpenGVLab/InternVideo.svg?style=social&label=Star)](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid) | [![Website](https://img.shields.io/badge/Website-9cf)](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid) | ICLR 2024 |
| [HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation](https://proceedings.neurips.cc/paper_files/paper/2024/file/23f3a0f82d79d985b6076bc84d14f66b-Paper-Datasets_and_Benchmarks_Track.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.17438) | [![Star](https://img.shields.io/github/stars/zhenzhiwang/HumanVid.svg?style=social&label=Star)](https://github.com/zhenzhiwang/HumanVid) | [![Website](https://img.shields.io/badge/Website-9cf)](https://humanvid.github.io/) | NeurIPS D&B 2024 |
| [Generating 3D-Consistent Videos from Unposed Internet Photos](https://arxiv.org/abs/2411.13549) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.13549) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://genechou.com/kfcw/) | CVPR 2025 |
| [CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation](https://arxiv.org/abs/2502.08639) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2502.08639) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://cinemaster-dev.github.io/) | - |


### Architecture for 3D diffusion models



| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [MVDream: Multi-view Diffusion for 3D Generation](https://openreview.net/pdf?id=FUgrjq2pbB) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/pdf/2308.16512) | [![Star](https://img.shields.io/github/stars/bytedance/MVDream.svg?style=social&label=Star)](https://github.com/bytedance/MVDream) | [![Website](https://img.shields.io/badge/Website-9cf)](https://mv-dream.github.io/) | ICLR 2024 |
| [MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion](https://proceedings.neurips.cc/paper_files/paper/2023/file/a0da690a47b2f52faa63f6fe054057b5-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2307.01097) | [![Star](https://img.shields.io/github/stars/Tangshitao/MVDiffusion.svg?style=social&label=Star)](https://github.com/Tangshitao/MVDiffusion) | [![Website](https://img.shields.io/badge/Website-9cf)](https://mvdiffusion.github.io/) | NeuIPS 2023 |
| [Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion](https://arxiv.org/abs/2404.06429) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.06429) | [![Star](https://img.shields.io/github/stars/magic-research/magic-boost.svg?style=social&label=Star)](https://github.com/magic-research/magic-boost) | [![Website](https://img.shields.io/badge/Website-9cf)](https://magic-research.github.io/magic-boost/) | - |
| [Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation](https://dl.acm.org/doi/10.1007/978-3-031-72907-2_23) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.09625) | [![Star](https://img.shields.io/github/stars/liuff19/Make-Your-3D.svg?style=social&label=Star)](https://github.com/liuff19/Make-Your-3D) | [![Website](https://img.shields.io/badge/Website-9cf)](https://liuff19.github.io/Make-Your-3D/) | ECCV 2024 |
| [UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00698.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.08754) | - | [![Website](https://img.shields.io/badge/Website-9cf)](/) | ECCV 2024 |
| [MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_MVD-Fusion_Single-view_3D_via_Depth-consistent_Multi-view_Generation_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.03656) | [![Star](https://img.shields.io/github/stars/zhizdev/mvdfusion.svg?style=social&label=Star)](https://github.com/zhizdev/mvdfusion) | [![Website](https://img.shields.io/badge/Website-9cf)](https://mvd-fusion.github.io/) | CVPR 2024 |
| [SPAD: Spatially Aware Multi-View Diffusers](https://openaccess.thecvf.com/content/CVPR2024/papers/Kant_SPAD_Spatially_Aware_Multi-View_Diffusers_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.05235) | [![Star](https://img.shields.io/github/stars/yashkant/spad.svg?style=social&label=Star)](https://github.com/yashkant/spad) | [![Website](https://img.shields.io/badge/Website-9cf)](https://yashkant.github.io/spad/) | CVPR 2024 |
| [ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_ConsistNet_Enforcing_3D_Consistency_for_Multi-view_Images_Diffusion_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2310.10343) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://jiayuyang.github.io/Consist_Net/) | CVPR 2024 |
| [ControlDreamer: Blending Geometry and Style in Text-to-3D](https://bmva-archive.org.uk/bmvc/2024/papers/Paper_74/paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.01129) | [![Star](https://img.shields.io/github/stars/oyt9306/ControlDreamer.svg?style=social&label=Star)](https://github.com/oyt9306/ControlDreamer) | [![Website](https://img.shields.io/badge/Website-9cf)](https://controldreamer.github.io/) | BMVC 2024 |
| [MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02446.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.12712) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://mvdiffusion-plusplus.github.io/) | ECCV 2024 |
| [CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation](https://arxiv.org/abs/2406.02509) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.02509) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://ir1d.github.io/CamCo/) | - |
| [EpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_EpiDiff_Enhancing_Multi-View_Synthesis_via_Localized_Epipolar-Constrained_Diffusion_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.06725) | [![Star](https://img.shields.io/github/stars/huanngzh/EpiDiff.svg?style=social&label=Star)](https://github.com/huanngzh/EpiDiff) | [![Website](https://img.shields.io/badge/Website-9cf)](https://huanngzh.github.io/EpiDiff/) | CVPR 2024 |
| [CamI2V: Camera-Controlled Image-to-Video Diffusion Model](https://arxiv.org/abs/2410.15957) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.15957) | [![Star](https://img.shields.io/github/stars/ZGCTroy/CamI2V.svg?style=social&label=Star)](https://github.com/ZGCTroy/CamI2V) | [![Website](https://img.shields.io/badge/Website-9cf)](https://zgctroy.github.io/CamI2V/) | - |
| [Vivid-ZOO: Multi-View Video Generation with Diffusion Model](https://proceedings.neurips.cc/paper_files/paper/2024/file/71c9eb0913e6c7fda3afd69c914b1a0c-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.08659) | [![Star](https://img.shields.io/github/stars/hi-zhengcheng/vividzoo.svg?style=social&label=Star)](https://github.com/hi-zhengcheng/vividzoo) | [![Website](https://img.shields.io/badge/Website-9cf)](https://hi-zhengcheng.github.io/vividzoo/) | NeurIPS 2024 |
| [SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency](https://openreview.net/pdf?id=tJoS2d0Onf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.17470) | [![Star](https://img.shields.io/github/stars/Stability-AI/generative-models.svg?style=social&label=Star)](https://github.com/Stability-AI/generative-models) | [![Website](https://img.shields.io/badge/Website-9cf)](https://sv4d.github.io/) | ICLR 2025 |
| [Cavia: Camera-controllable multi-view video diffusion with view-integrated attention](https://arxiv.org/abs/2410.10774) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.10774) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://ir1d.github.io/Cavia/) | - |
| [CAT3D: Create Anything in 3D with Multi-View Diffusion Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/89e4433fec4b99f1d859db57af1e0a0f-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.10314) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://cat3d.github.io/) | NeurIPS 2024 |
| [Generating 3D-Consistent Videos from Unposed Internet Photos](https://arxiv.org/abs/2411.13549) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.13549) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://genechou.com/kfcw/) | CVPR 2025 |
| [Wonderland: Navigating 3D Scenes from a Single Image](https://arxiv.org/abs/2412.12091) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.12091) | [![Star](https://img.shields.io/github/stars/snap-research/wonderland.svg?style=social&label=Star)](https://github.com/snap-research/wonderland/) | [![Website](https://img.shields.io/badge/Website-9cf)](https://snap-research.github.io/wonderland/index.html) | - |
| [Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control](https://arxiv.org/abs/2501.03847) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2501.03847) | [![Star](https://img.shields.io/github/stars/IGL-HKUST/DiffusionAsShader.svg?style=social&label=Star)](https://github.com/IGL-HKUST/DiffusionAsShader) | [![Website](https://img.shields.io/badge/Website-9cf)](https://igl-hkust.github.io/das/) | - |
| [CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation](https://arxiv.org/abs/2502.08639) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2502.08639) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://cinemaster-dev.github.io/) | - |


### Camera conditioning


| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00150.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.12008) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://sv3d.github.io/) | ECCV 2024 |
| [Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models](https://openreview.net/pdf?id=VEHNTupyIU) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2409.07452) | [![Star](https://img.shields.io/github/stars/yanghb22-fdu/Hi3D-Official.svg?style=social&label=Star)](https://github.com/yanghb22-fdu/Hi3D-Official) | [![Website](https://img.shields.io/badge/Website-9cf)](https://github.com/yanghb22-fdu/Hi3D-Official) | Multimedia 2024 |
| [Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion](https://dl.acm.org/doi/10.1145/3641519.3657481) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.03162) | [![Star](https://img.shields.io/github/stars/ysy31415/direct_a_video.svg?style=social&label=Star)](https://github.com/ysy31415/direct_a_video) | [![Website](https://img.shields.io/badge/Website-9cf)](https://direct-a-video.github.io/) | SIGGRAPH 2024 |
| [Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03510.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.14868) | [![Star](https://img.shields.io/github/stars/basilevh/gcd.svg?style=social&label=Star)](https://github.com/basilevh/gcd) | [![Website](https://img.shields.io/badge/Website-9cf)](https://gcd.cs.columbia.edu/) | ECCV 2024 |
| [MotionCtrl: A Unified and Flexible Motion Controller for Video Generation](https://dl.acm.org/doi/10.1145/3641519.3657518) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.03641) | [![Star](https://img.shields.io/github/stars/TencentARC/MotionCtrl.svg?style=social&label=Star)](https://github.com/TencentARC/MotionCtrl) | [![Website](https://img.shields.io/badge/Website-9cf)](https://wzhouxiff.github.io/projects/MotionCtrl/) | SIGGRAPH 2024 |
| [CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation](https://arxiv.org/abs/2502.08639) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2502.08639) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://cinemaster-dev.github.io/) | - |
| [CAT3D: Create Anything in 3D with Multi-View Diffusion Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/89e4433fec4b99f1d859db57af1e0a0f-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.10314) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://cat3d.github.io/) | NeurIPS 2024 |
| [Controlling Space and Time with Diffusion Models](https://openreview.net/pdf?id=d2UrCGtntF) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.07860) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://4d-diffusion.github.io/) | ICLR 2025 |
| [CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models](https://arxiv.org/abs/2411.18613) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.18613) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://cat-4d.github.io/) | - |
| [CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation](https://arxiv.org/abs/2406.02509) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.02509) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://ir1d.github.io/CamCo/) | - |
| [Cavia: Camera-controllable multi-view video diffusion with view-integrated attention](https://arxiv.org/abs/2410.10774) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.10774) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://ir1d.github.io/Cavia/) | - |
| [Vd3d: Taming large video diffusion transformers for 3d camera control](https://openreview.net/pdf?id=0n4bS0R5MM) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.12781) | [![Star](https://img.shields.io/github/stars/snap-research/ac3d.svg?style=social&label=Star)](https://github.com/snap-research/ac3d) | [![Website](https://img.shields.io/badge/Website-9cf)](https://snap-research.github.io/vd3d/) | ICLR 2025 |
| [AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers](https://arxiv.org/abs/2411.18673) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.18673) | [![Star](https://img.shields.io/github/stars/snap-research/ac3d.svg?style=social&label=Star)](https://github.com/snap-research/ac3d) | [![Website](https://img.shields.io/badge/Website-9cf)](https://snap-research.github.io/ac3d/) | CVPR 2025 |
| [Cameractrl: Enabling camera control for text-to-video generation](https://openreview.net/pdf?id=Z4evOUYrk7) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.02101) | [![Star](https://img.shields.io/github/stars/hehao13/CameraCtrl.svg?style=social&label=Star)](https://github.com/hehao13/CameraCtrl) | [![Website](https://img.shields.io/badge/Website-9cf)](https://hehao13.github.io/projects-CameraCtrl/) | ICLR 2025 |
| [Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control](https://proceedings.neurips.cc/paper_files/paper/2024/file/1d49235669869ab737c1da9d64b7c769-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.17414) | [![Star](https://img.shields.io/github/stars/CollaborativeVideoDiffusion/CVD.svg?style=social&label=Star)](https://github.com/CollaborativeVideoDiffusion/CVD) | [![Website](https://img.shields.io/badge/Website-9cf)](https://collaborativevideodiffusion.github.io/) | NeurIPS 2024 |
| [MyGo: Consistent and Controllable Multi-View Driving Video Generation with Camera Control](https://arxiv.org/abs/2409.06189) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2409.06189) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://metadrivescape.github.io/papers_project/MyGo/page.html) | - |
| [HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation](https://openreview.net/pdf?id=catfRXDWcb) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.17438) | [![Star](https://img.shields.io/github/stars/zhenzhiwang/HumanVid.svg?style=social&label=Star)](https://github.com/zhenzhiwang/HumanVid) | [![Website](https://img.shields.io/badge/Website-9cf)](https://humanvid.github.io/) | NeurIPS D&B 2024 |
| [CamI2V: Camera-Controlled Image-to-Video Diffusion Model](https://arxiv.org/abs/2410.15957) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.15957) | [![Star](https://img.shields.io/github/stars/ZGCTroy/CamI2V.svg?style=social&label=Star)](https://github.com/ZGCTroy/CamI2V) | [![Website](https://img.shields.io/badge/Website-9cf)](https://zgctroy.github.io/CamI2V/) | - |
| [Wonderland: Navigating 3D Scenes from a Single Image](https://arxiv.org/abs/2412.12091) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.12091) | [![Star](https://img.shields.io/github/stars/snap-research/wonderland.svg?style=social&label=Star)](https://github.com/snap-research/wonderland/) | [![Website](https://img.shields.io/badge/Website-9cf)](https://snap-research.github.io/wonderland/index.html) | - |
| [I2VControl-Camera: Precise Video Camera Control with Adjustable Motion Strength](https://openreview.net/pdf?id=AcAD4VEgCX) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.06525) | [![Star](https://img.shields.io/github/stars/WanquanF/I2VControl-Camera.svg?style=social&label=Star)](https://github.com/WanquanF/I2VControl-Camera) | [![Website](https://img.shields.io/badge/Website-9cf)](https://wanquanf.github.io/I2VControlCamera) | ICLR 2025 |
| [Motion Prompting: Controlling Video Generation with Motion Trajectories](https://arxiv.org/abs/2412.02700) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.02700) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://motion-prompting.github.io/) | CVPR 2025 |
| [Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control](https://arxiv.org/abs/2501.03847) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2501.03847) | [![Star](https://img.shields.io/github/stars/IGL-HKUST/DiffusionAsShader.svg?style=social&label=Star)](https://github.com/IGL-HKUST/DiffusionAsShader) | [![Website](https://img.shields.io/badge/Website-9cf)](https://igl-hkust.github.io/das/) | - |


### Inference-time tricks

| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Kwak_ViVid-1-to-3_Novel_View_Synthesis_with_Video_Diffusion_Models_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.01305) | [![Star](https://img.shields.io/github/stars/ubc-vision/vivid123.svg?style=social&label=Star)](https://github.com/ubc-vision/vivid123) | [![Website](https://img.shields.io/badge/Website-9cf)](https://jgkwak95.github.io/ViVid-1-to-3/) | CVPR 2024 |
| [NVS-Solver: Video Diffusion Model as Zero-Shot Novel View Synthesizer](https://openreview.net/pdf?id=zDJf7fvdid) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.15364) | [![Star](https://img.shields.io/github/stars/ZHU-Zhiyu/NVS_Solver.svg?style=social&label=Star)](https://github.com/ZHU-Zhiyu/NVS_Solver) | [![Website](https://img.shields.io/badge/Website-9cf)](https://github.com/ZHU-Zhiyu/NVS_Solver) | ICLR 2025 |
| [Training-free Camera Control for Video Generation](https://openreview.net/pdf?id=KI1zldOFz9) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.10126) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://lifedecoder.github.io/CamTrol/) | ICLR 2025 |

# Benefits to other domains

## Video representation learning

## Video retrieval

## Video QA and captioning

## 3D and 4D generation

### Video diffusion for 3D generation


| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [V3D: Video Diffusion Models are Effective 3D Generators](https://arxiv.org/abs/2403.06738) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.06738) | [![Star](https://img.shields.io/github/stars/heheyas/V3D.svg?style=social&label=Star)](https://github.com/heheyas/V3D) | [![Website](https://img.shields.io/badge/Website-9cf)](https://heheyas.github.io/V3D/) | - |
| [Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00150.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.12008) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://sv3d.github.io/) | ECCV 2024 |
| [IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation](https://dl.acm.org/doi/10.5555/3692070.3693507) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.08682) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://lukemelas.github.io/IM-3D/) | ICML 2024 |
| [VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00255.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.12034) | [![Star](https://img.shields.io/github/stars/facebookresearch/vfusion3d.svg?style=social&label=Star)](https://github.com/facebookresearch/vfusion3d) | [![Website](https://img.shields.io/badge/Website-9cf)](https://junlinhan.github.io/projects/vfusion3d.html) | ECCV 2024 |
| [Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models](https://openreview.net/pdf?id=VEHNTupyIU) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2409.07452) | [![Star](https://img.shields.io/github/stars/yanghb22-fdu/Hi3D-Official.svg?style=social&label=Star)](https://github.com/yanghb22-fdu/Hi3D-Official) | [![Website](https://img.shields.io/badge/Website-9cf)](https://github.com/yanghb22-fdu/Hi3D-Official) | Multimedia 2024 |
| [ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model](https://arxiv.org/abs/2408.16767) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2408.16767) | [![Star](https://img.shields.io/github/stars/liuff19/ReconX.svg?style=social&label=Star)](https://github.com/liuff19/ReconX) | [![Website](https://img.shields.io/badge/Website-9cf)](https://liuff19.github.io/ReconX/) | - |
| [Wonderland: Navigating 3D Scenes from a Single Image](https://arxiv.org/abs/2412.12091) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.12091) | [![Star](https://img.shields.io/github/stars/snap-research/wonderland.svg?style=social&label=Star)](https://github.com/snap-research/wonderland/) | [![Website](https://img.shields.io/badge/Website-9cf)](https://snap-research.github.io/wonderland/index.html) | - |


### Video diffusion for 4D generation


| Title | arXiv | Github| WebSite | Pub. & Date
|:-----|:-----:|:-----:|:-----:|:-----:|
| [Text-to-4d dynamic scene generation](https://proceedings.mlr.press/v202/singer23a/singer23a.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2301.11280) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://make-a-video3d.github.io/) | ICML 2023 |
| [4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling](https://openaccess.thecvf.com/content/CVPR2024/papers/Bahmani_4D-fy_Text-to-4D_Generation_Using_Hybrid_Score_Distillation_Sampling_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.17984) | [![Star](https://img.shields.io/github/stars/sherwinbahmani/4dfy.svg?style=social&label=Star)](https://github.com/sherwinbahmani/4dfy) | [![Website](https://img.shields.io/badge/Website-9cf)](https://sherwinbahmani.github.io/4dfy/) | CVPR 2024 |
| [Dream-in-4D: A Unified Approach for Text-and Image-Guided 4D Scene Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_A_Unified_Approach_for_Text-_and_Image-guided_4D_Scene_Generation_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.16854) | [![Star](https://img.shields.io/github/stars/NVlabs/dream-in-4d.svg?style=social&label=Star)](https://github.com/NVlabs/dream-in-4d) | [![Website](https://img.shields.io/badge/Website-9cf)](https://research.nvidia.com/labs/nxp/dream-in-4d/) | CVPR 2024 |
| [Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Ling_Align_Your_Gaussians_Text-to-4D_with_Dynamic_3D_Gaussians_and_Composed_CVPR_2024_paper.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.13763) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/) | CVPR 2024 |
| [Animate124: Animating One Image to 4D Dynamic Scene](https://arxiv.org/abs/2311.14603) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.14603) | [![Star](https://img.shields.io/github/stars/HeliosZhao/Animate124.svg?style=social&label=Star)](https://github.com/HeliosZhao/Animate124) | [![Website](https://img.shields.io/badge/Website-9cf)](https://animate124.github.io/) | - |
| [Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels](https://proceedings.neurips.cc/paper_files/paper/2024/file/ed3c686f9cda57e56cc859402c775414-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.16822) | [![Star](https://img.shields.io/github/stars/yikaiw/vidu4d.svg?style=social&label=Star)](https://github.com/yikaiw/vidu4d) | [![Website](https://img.shields.io/badge/Website-9cf)](https://vidu4d-dgs.github.io/) | NeurIPS 2024 |
| [STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05288.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.14939) | [![Star](https://img.shields.io/github/stars/zeng-yifei/STAG4D.svg?style=social&label=Star)](https://github.com/zeng-yifei/STAG4D) | [![Website](https://img.shields.io/badge/Website-9cf)](https://nju-3dv.github.io/projects/STAG4D/) | ECCV 2024 |
| [PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting](https://arxiv.org/abs/2405.19957) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.19957) | - | [![Website](https://img.shields.io/badge/Website-9cf)](/) | - |
| [TC4D: Trajectory-Conditioned Text-to-4D Generation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06234.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.17920) | [![Star](https://img.shields.io/github/stars/sherwinbahmani/tc4d.svg?style=social&label=Star)](https://github.com/sherwinbahmani/tc4d) | [![Website](https://img.shields.io/badge/Website-9cf)](https://sherwinbahmani.github.io/tc4d/) | ECCV 2024 |
| [EG4D: Explicit Generation of 4D Object without Score Distillation](https://openreview.net/pdf/9f524271794649459a528b3b78faee7a6341a7a7.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.18132) | [![Star](https://img.shields.io/github/stars/jasongzy/EG4D.svg?style=social&label=Star)](https://github.com/jasongzy/EG4D) | [![Website](https://img.shields.io/badge/Website-9cf)](https://github.com/jasongzy/EG4D) | ICLR 2025 |
| [4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/50358459632f7fc1c7e9f9f0ad0cc026-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.07472) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://snap-research.github.io/4Real/) | NeurIPS 2024 |
| [DreamGaussian4D: Generative 4D Gaussian Splatting](https://arxiv.org/abs/2312.17142) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.17142) | [![Star](https://img.shields.io/github/stars/jiawei-ren/dreamgaussian4d.svg?style=social&label=Star)](https://github.com/jiawei-ren/dreamgaussian4d) | [![Website](https://img.shields.io/badge/Website-9cf)](https://jiawei-ren.github.io/projects/dreamgaussian4d/) | - |
| [Diffusion4D: Fast Spatial-temporal Consistent 4D generation via Video Diffusion Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/c7f4dbb8f3739b36029ba71a47844696-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.16645) | [![Star](https://img.shields.io/github/stars/VITA-Group/Diffusion4D.svg?style=social&label=Star)](https://github.com/VITA-Group/Diffusion4D) | [![Website](https://img.shields.io/badge/Website-9cf)](https://vita-group.github.io/Diffusion4D/) | NeurIPS 2024 |
| [4Diffusion: Multi-view Video Diffusion Model for 4D Generation](https://proceedings.neurips.cc/paper_files/paper/2024/file/1bbfea488a8968e2d3c6565639b08e5e-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.20674) | [![Star](https://img.shields.io/github/stars/aejion/4Diffusion.svg?style=social&label=Star)](https://github.com/aejion/4Diffusion) | [![Website](https://img.shields.io/badge/Website-9cf)](https://aejion.github.io/4diffusion/) | NeurIPS 2024 |
| [Animate3D: Animating Any 3D Model with Multi-view Video Diffusion](https://proceedings.neurips.cc/paper_files/paper/2024/file/e3b53f89136b1bc69a5714ea465f01b6-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.11398) | [![Star](https://img.shields.io/github/stars/yanqinJiang/Animate3D.svg?style=social&label=Star)](https://github.com/yanqinJiang/Animate3D) | [![Website](https://img.shields.io/badge/Website-9cf)](https://animate3d.github.io/) | NeurIPS 2024 |
| [VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model](https://arxiv.org/abs/2403.12010) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.12010) | [![Star](https://img.shields.io/github/stars/alibaba/VideoMV.svg?style=social&label=Star)](https://github.com/alibaba/VideoMV) | [![Website](https://img.shields.io/badge/Website-9cf)](https://aigc3d.github.io/VideoMV/) | - |
| [Vivid-ZOO: Multi-View Video Generation with Diffusion Model](https://proceedings.neurips.cc/paper_files/paper/2024/file/71c9eb0913e6c7fda3afd69c914b1a0c-Paper-Conference.pdf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.08659) | [![Star](https://img.shields.io/github/stars/hi-zhengcheng/vividzoo.svg?style=social&label=Star)](https://github.com/hi-zhengcheng/vividzoo) | [![Website](https://img.shields.io/badge/Website-9cf)](https://hi-zhengcheng.github.io/vividzoo/) | NeurIPS 2024 |
| [SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency](https://openreview.net/pdf?id=tJoS2d0Onf) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.17470) | [![Star](https://img.shields.io/github/stars/Stability-AI/generative-models.svg?style=social&label=Star)](https://github.com/Stability-AI/generative-models) | [![Website](https://img.shields.io/badge/Website-9cf)](https://sv4d.github.io/) | ICLR 2025 |
| [CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models](https://arxiv.org/abs/2411.18613) | [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.18613) | - | [![Website](https://img.shields.io/badge/Website-9cf)](https://cat-4d.github.io/) | - |

## Citation
If you find our survey is useful in your research or applications, please consider giving us a star 🌟 and citing it by the following BibTeX entry.

```
To update
```

## Acknowledgement
The format of this repo is built based on [Awesome-Video-Diffusion-Models](https://github.com/ChenHsing/Awesome-Video-Diffusion-Models).
